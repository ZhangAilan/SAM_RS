{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43065,"status":"ok","timestamp":1730216941617,"user":{"displayName":"Ailan Zhang","userId":"06645751235937646570"},"user_tz":-480},"id":"MpxLxVy68uW0","outputId":"eca70ee6-96bb-4503-eab7-4dc0e9a9d1c7"},"outputs":[],"source":["%pip install git+https://github.com/facebookresearch/segment-anything.git\n","%pip install -q git+https://github.com/huggingface/transformers.git\n","%pip install datasets\n","%pip install numpy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from datasets import load_from_disk\n","#路径\n","dataset = load_from_disk(\"/content/drive/MyDrive/sam_rs/temp\")  #加载数据集\n","save_path = \"/content/drive/MyDrive/sam_rs/models\"  #保存模型的路径\n","loop_times = 1  #训练次数\n","batch_size = 4  #批处理大小"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f34nV5kv7bq7"},"outputs":[],"source":["import numpy as np\n","from torch.utils.data import Dataset\n","from transformers import SamProcessor\n","from torch.utils.data import DataLoader\n","from transformers import SamModel\n","import torch\n","from torch.optim import Adam\n","import torch.nn as nn\n","from tqdm import tqdm\n","from statistics import mean\n","\n","#检查加载的数据集\n","print(dataset)\n","print('number of samples:',len(dataset))\n","\n","#Get bounding boxes from mask. 作为prompt的一部分，我们将使用mask来获取bounding box。\n","def get_bounding_box(ground_truth_map):\n","  # get bounding box from mask\n","  y_indices, x_indices = np.where(ground_truth_map > 0)\n","  x_min, x_max = np.min(x_indices), np.max(x_indices)\n","  y_min, y_max = np.min(y_indices), np.max(y_indices)\n","  # add perturbation to bounding box coordinates\n","  H, W = ground_truth_map.shape\n","  x_min = max(0, x_min - np.random.randint(0, 20))\n","  x_max = min(W, x_max + np.random.randint(0, 20))\n","  y_min = max(0, y_min - np.random.randint(0, 20))\n","  y_max = min(H, y_max + np.random.randint(0, 20))\n","  bbox = [x_min, y_min, x_max, y_max]\n","  return bbox\n","\n","class SAMDataset(Dataset):\n","  \"\"\"\n","  This class is used to create a dataset that serves input images and masks.\n","  It takes a dataset and a processor as input and overrides the __len__ and __getitem__ methods of the Dataset class.\n","  \"\"\"\n","  def __init__(self, dataset, processor):\n","    self.dataset = dataset\n","    self.processor = processor\n","\n","  def __len__(self):\n","    return len(self.dataset)\n","\n","  def __getitem__(self, idx):\n","    item = self.dataset[idx]\n","    image = np.array(item[\"image\"])  # 将 PIL 图像转换为 NumPy 数组\n","    ground_truth_mask = np.array(item[\"label\"])\n","\n","    # 检查图像维度，如果是灰度图像则添加通道维度\n","    if image.ndim == 2:\n","        image = np.stack((image,) * 3, axis=-1)  # 将 (height, width) 转换为 (height, width, 3) 三通道灰度图\n","\n","    # get bounding box prompt\n","    prompt = get_bounding_box(ground_truth_mask)\n","    # prepare image and prompt for the model\n","    inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n","    # remove batch dimension which the processor adds by default\n","    inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n","    # add ground truth segmentation\n","    inputs[\"ground_truth_mask\"] = ground_truth_mask\n","    return inputs\n","\n","# Initialize the processor\n","processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n","\n","# Create an instance of the SAMDataset\n","train_dataset = SAMDataset(dataset=dataset, processor=processor)\n","example = train_dataset[0]\n","print(\"train_dataset第一个样本：\")\n","for k,v in example.items():\n","  print(k,v.shape)\n","\n","# Create a DataLoader instance for the training dataset\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n","batch = next(iter(train_dataloader))\n","print(\"每次批处理的样本:\")\n","for k,v in batch.items():\n","  print(k,v.shape)\n","\n","# Load the model\n","model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n","# make sure we only compute gradients for mask decoder\n","for name, param in model.named_parameters():\n","  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n","    param.requires_grad_(False)\n","\n","# Initialize the optimizer and the loss function\n","optimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n","# 这里选择交叉熵损失（适用于多类分割任务）\n","seg_loss = nn.CrossEntropyLoss()\n","\n","#Training loop\n","num_epochs = loop_times\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model.to(device)\n","print(f\"Training on {device}\")\n","\n","model.train()\n","print(\"Start training...\")\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch + 1}\")\n","    epoch_losses = []\n","    for batch in tqdm(train_dataloader):\n","      # forward pass\n","      outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n","                      input_boxes=batch[\"input_boxes\"].to(device),\n","                      multimask_output=False)\n","\n","      # compute loss\n","      predicted_masks = outputs.pred_masks.squeeze(1)\n","      ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n","      loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n","\n","      # backward pass (compute gradients of parameters w.r.t. loss)\n","      optimizer.zero_grad()\n","      loss.backward()\n","\n","      # optimize\n","      optimizer.step()\n","      epoch_losses.append(loss.item())\n","\n","    print(f'EPOCH: {epoch}')\n","    print(f'Mean loss: {mean(epoch_losses)}')\n","\n","# Save the model's state dictionary to a file\n","torch.save(model.state_dict(), save_path + \"/sam_model.pth\")\n","print(\"Model saved successfully!\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNbZSmsKKweY0JQnCmi2OXW","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
